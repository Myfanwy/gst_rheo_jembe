{
    "collab_server" : "",
    "contents" : "---\ntitle: 'Frequentist Analysis: Experiment 1'\nauthor: \"Myfanwy Johnston, Neil Willits\"\ndate: \"August 23, 2016\"\noutput: pdf_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n\n```\n\n## Data Exploration, Experiment 1\n\nFirst, a summary of the response variable and a plot of its distribution:\n\n```{r datachunk, cache=TRUE, echo=FALSE, message=FALSE}\nd <- read.csv(\"~/Dropbox/GitHubRepos/gst_rheo/data_tidy/gst_rheo_all.csv\", header=T, stringsAsFactors = F)\nd <- d[d$Type == \"with\", ]\nd <- d[d$TrialNo != 15, ] # exclude 2nd trial of fish #20.\nsummary(d$PropPos)\nplot(density(d$PropPos), main=\"PropPos\")\n```\n\n## Modeling Experiment 1\n\nThese experiments were originally conceived with a Latin Square design: 24 fish and 4 Treatments = 6 Latin Squares.  A Latin Square design makes very efficient use of the individuals (fish, in this case) that are in an analysis, since by using a fish multiple times and partitioning off a (categorical) fish effect, the error term we're using represents within-fish variability, which is apt to be considerably smaller than the between-fish variability that would be used as an error term in a randomized block design. However, to do this, we need to assume that there are no orderXtreatment or orderXfish effects, which is a fairly strong assumption, and so it should be checked as part of running this type of analysis.  Additionally, we need to exclude the one fish of the 23 that was run through each treatment twice, so that we don't face issues with pseudoreplication.\n\nAfter first running the model on the raw response variable, the residuals failed a Shapiro-Wilks test (W = 0.93).  As a result, we've used the following transformation of the response before continuing:\n\n\n```{r transform, cache=TRUE}\n## Transform PropPos \nd$PropPosTransform <- log((d$PropPos + .005)/(1-d$PropPos + .005))\n\n```\n\nNow we can construct the model and perform tests of our assumptions that the residual errors are normal, and that the assumption of constant variance is valid (using a Levene test).  The model structure is:\n\nY ~ square/fish + order + treatment\n\nWhere Y is our response variable (proportion of time spent positively oriented in a given treatment), square is the Latin Square to which that trial belongs (included to account for temporal variation), nested within individual fish (so that the model accounts for within-fish variation across squares).  Order and Treatment are then included as main effects.\n\n\n```{r modelprep, cache=TRUE, echo=FALSE}\n\nd$FishID2 <- as.factor(d$FishID)\nd$Order2 <- as.factor(d$Order)\nd$TrialNo2 <- as.factor(d$TrialNo)\nd$Square <- as.factor(as.integer((d$TrialNo + 3.5)/4))\nd$Treatment <- as.factor(d$Treatment)\n```\n\n```{r model, cache=TRUE}\nm1 <- lm(PropPosTransform ~ Square/FishID2 + Order2 + Treatment, data = d)\n\nanova(m1)\nshapiro.test(m1$residuals)\n\n```\n\nWe consider this result to be acceptable, since the value of the test statistic is high (W > .95, which is a somewhat arbitrary cutoff). The test statistic is equivalent to a correlation that’s based on a normal probability plot, so when this value is close to one, it means that there’s a close correspondence between the error distribution and a normal distribution. That’s where the Central Limit Theorem comes in, since with a W-value this high, the ANOVA’s F-statistics will behave as if the data were normal, even if they’re not exactly normal.\n\n```{r Levene, cache=TRUE}\n# run a levene test:  \nm1Levene <- lm(abs(m1$residuals) ~ Square/FishID2 + Order2 + Treatment, data = d)\nanova(m1Levene) # slight significance; means that there is more variance in some treatments than others. \n```\n\nThe output of the Levene test is an ANOVA table, which contains a series of p-values (one for each term in the model). This causes the overall error rate to be inflated, so we will use a Bonferroni adjustment, comparing each p-value against .05/k, where k is the number of terms in the ANOVA table:\n\n```{r Bonferroni}\n0.05/4 \n```\n\nAt this new significance level, the only terms that change are the Square and Square/FishID term, which become slightly more significant.\n\nAt this point, we have satisfied normality and constant variance assumptions.  Treatment, Order, and variation due to temporal variability (represented by Square) and individual fish remain significant predictors of the time spent positively oriented during a given trial.\n\n",
    "created" : 1474910938390.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1604735731",
    "id" : "7E93665A",
    "lastKnownWriteTime" : 1474911345,
    "last_content_update" : 1474911345278,
    "path" : "~/Dropbox/GitHubRepos/gst_rheo/manuscript/supp_materials/FrequentistApproach/FrequentistAnalysis.Rmd",
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}